{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855995d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset\n",
    "from utils import get_data, get_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc0b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63af47ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"intfloat/e5-mistral-7b-instruct\")\n",
    "model = AutoModel.from_pretrained(\"intfloat/e5-mistral-7b-instruct\", device_map=\"auto\")\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9080fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"kuznetsoffandrey/sberquad\")\n",
    "train_data = [{\"question\": q, \"context\": c} for q, c in zip(dataset[\"train\"][\"question\"], dataset[\"train\"][\"context\"]) if c is not None]\n",
    "queries_train, passages_train = get_data(range(len(train_data)), train_data)\n",
    "train_data_batched = get_batches(queries_train, passages_train, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1609c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_token_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "\n",
    "\n",
    "def hard_negative_mining(batch_index: int, query: str, positive_doc: str, candidate_docs: list, \n",
    "                            task: str, margin: float = 0.95, top_k: int = 5):\n",
    "    query_text = get_detailed_instruct(task, query)\n",
    "    input_texts = [query_text, positive_doc] + candidate_docs\n",
    "\n",
    "    batch_dict = tokenizer(input_texts, max_length=4096, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch_dict)\n",
    "    embeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    q_emb = embeddings[0:1]\n",
    "    pos_emb = embeddings[1:2]\n",
    "    cand_embs = embeddings[2:]\n",
    "\n",
    "    pos_score = (q_emb @ pos_emb.T).item()\n",
    "    scores = (q_emb @ cand_embs.T).squeeze(0)\n",
    "\n",
    "    threshold = pos_score * margin\n",
    "    mask = scores < threshold\n",
    "    filtered_scores = scores[mask]\n",
    "    filtered_indices = torch.arange(len(candidate_docs))[mask]\n",
    "\n",
    "    # top-k самых трудных\n",
    "    if len(filtered_scores) > 0:\n",
    "        topk = torch.topk(filtered_scores, min(top_k, len(filtered_scores)))[1]\n",
    "        hard_negatives = [candidate_docs[idx] for idx in filtered_indices[topk]]\n",
    "    else:\n",
    "        hard_negatives = []\n",
    "\n",
    "    return {\n",
    "        \"batch_index\": batch_index,\n",
    "        \"query\": query,\n",
    "        \"positive\": positive_doc,\n",
    "        \"hard_negatives\": hard_negatives\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566906e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "output_file = \"hardneg_dataset.jsonl\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for batch_index, batch in enumerate(train_data_batched):\n",
    "        for idx in range(BATCH_SIZE):\n",
    "            query = batch[\"question\"][idx]\n",
    "            positive_doc = batch[\"context\"][idx]\n",
    "            candidate_docs = batch[\"context\"][0:idx] + batch[\"context\"][idx + 1:BATCH_SIZE]\n",
    "            \n",
    "            result = hard_negative_mining(batch_index, query, positive_doc, candidate_docs, task, margin=0.95, top_k=5)\n",
    "\n",
    "            f.write(json.dumps(result, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99702910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
